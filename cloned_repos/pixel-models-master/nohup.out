OPTIONS Namespace(batch_size=32, cache_dir='./cache', channels=60, data_dir='/home/ar5g15/workspace/imagenet_valid', embedding_size=300, epochs=31, kernel_size=7, latent_size=32, limit=None, lr=0.001, model='vae-up', no_gates=False, no_hv=False, no_res=False, num_layers=5, task='imagenet64', tb_dir='./runs/pixel', vae_depth=0, zsize=64)
vae-up
a fost
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
Constructed network ImEncoder(
  (encoder): Sequential(
    (0): Block(
      (upchannels): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
    (2): Block(
      (upchannels): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
    (4): Block(
      (upchannels): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
    (6): Flatten()
    (7): Linear(in_features=8192, out_features=128, bias=True)
  )
) ImDecoder(
  (decoder): Sequential(
    (0): Linear(in_features=64, out_features=8192, bias=True)
    (1): ReLU()
    (2): Reshape()
    (3): Upsample(scale_factor=2, mode=bilinear)
    (4): Block(
      (upchannels): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (5): Upsample(scale_factor=2, mode=bilinear)
    (6): Block(
      (upchannels): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (7): Upsample(scale_factor=2, mode=bilinear)
    (8): Block(
      (upchannels): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): ConvTranspose2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): ConvTranspose2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): ConvTranspose2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (9): ConvTranspose2d(60, 64, kernel_size=(1, 1), stride=(1, 1))
    (10): Sigmoid()
  )
) LGated(
  (conv1): Conv2d(3, 60, kernel_size=(1, 1), stride=(1, 1), groups=3)
  (gated_layers): ModuleList(
    (0): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (1): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (3): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (4): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (conv2): Conv2d(60, 768, kernel_size=(1, 1), stride=(1, 1), groups=3)
)
  0%|          | 0/1406 [00:00<?, ?it/s]/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.{} is deprecated. Use nn.functional.interpolate instead.".format(self.name))
/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  0%|          | 1/1406 [00:02<48:09,  2.06s/it]  0%|          | 2/1406 [00:03<44:14,  1.89s/it]  0%|          | 3/1406 [00:05<41:26,  1.77s/it]  0%|          | 4/1406 [00:06<39:26,  1.69s/it]  0%|          | 5/1406 [00:08<38:03,  1.63s/it]  0%|          | 6/1406 [00:09<37:09,  1.59s/it]  0%|          | 7/1406 [00:11<36:30,  1.57s/it]  1%|          | 8/1406 [00:12<36:01,  1.55s/it]  1%|          | 9/1406 [00:14<35:43,  1.53s/it]  1%|          | 10/1406 [00:15<35:28,  1.52s/it]  1%|          | 11/1406 [00:17<35:16,  1.52s/it]  1%|          | 12/1406 [00:18<35:08,  1.51s/it]  1%|          | 13/1406 [00:20<35:02,  1.51s/it]  1%|          | 14/1406 [00:21<34:55,  1.51s/it]  1%|          | 15/1406 [00:23<34:54,  1.51s/it]  1%|          | 16/1406 [00:24<34:50,  1.50s/it]  1%|          | 17/1406 [00:26<34:55,  1.51s/it]  1%|▏         | 18/1406 [00:27<34:54,  1.51s/it]  1%|▏         | 19/1406 [00:29<34:51,  1.51s/it]  1%|▏         | 20/1406 [00:30<34:49,  1.51s/it]  1%|▏         | 21/1406 [00:32<34:45,  1.51s/it]  2%|▏         | 22/1406 [00:33<34:42,  1.50s/it]  2%|▏         | 23/1406 [00:35<34:42,  1.51s/it]  2%|▏         | 24/1406 [00:36<34:40,  1.51s/it]  2%|▏         | 25/1406 [00:38<34:38,  1.51s/it]  2%|▏         | 26/1406 [00:39<34:36,  1.50s/it]  2%|▏         | 27/1406 [00:41<34:32,  1.50s/it]  2%|▏         | 28/1406 [00:42<34:30,  1.50s/it]  2%|▏         | 29/1406 [00:44<34:30,  1.50s/it]  2%|▏         | 30/1406 [00:45<34:28,  1.50s/it]  2%|▏         | 31/1406 [00:47<34:26,  1.50s/it]  2%|▏         | 32/1406 [00:48<34:24,  1.50s/it]  2%|▏         | 33/1406 [00:50<34:23,  1.50s/it]  2%|▏         | 34/1406 [00:51<34:22,  1.50s/it]  2%|▏         | 35/1406 [00:53<34:20,  1.50s/it]  3%|▎         | 36/1406 [00:54<34:16,  1.50s/it]  3%|▎         | 37/1406 [00:56<34:18,  1.50s/it]  3%|▎         | 38/1406 [00:57<34:19,  1.51s/it]  3%|▎         | 39/1406 [00:59<34:17,  1.51s/it]  3%|▎         | 40/1406 [01:00<34:15,  1.50s/it]  3%|▎         | 41/1406 [01:02<34:11,  1.50s/it]  3%|▎         | 42/1406 [01:03<34:16,  1.51s/it]  3%|▎         | 43/1406 [01:05<34:14,  1.51s/it]  3%|▎         | 44/1406 [01:06<34:10,  1.51s/it]  3%|▎         | 45/1406 [01:08<34:06,  1.50s/it]  3%|▎         | 46/1406 [01:09<34:05,  1.50s/it]  3%|▎         | 47/1406 [01:11<34:02,  1.50s/it]  3%|▎         | 48/1406 [01:12<34:00,  1.50s/it]  3%|▎         | 49/1406 [01:14<34:00,  1.50s/it]  4%|▎         | 50/1406 [01:15<33:59,  1.50s/it]  4%|▎         | 51/1406 [01:17<33:59,  1.51s/it]  4%|▎         | 52/1406 [01:18<33:56,  1.50s/it]  4%|▍         | 53/1406 [01:20<33:54,  1.50s/it]  4%|▍         | 54/1406 [01:21<34:03,  1.51s/it]  4%|▍         | 55/1406 [01:23<33:59,  1.51s/it]  4%|▍         | 56/1406 [01:24<33:56,  1.51s/it]  4%|▍         | 57/1406 [01:26<33:56,  1.51s/it]  4%|▍         | 58/1406 [01:27<33:49,  1.51s/it]  4%|▍         | 59/1406 [01:29<33:52,  1.51s/it]  4%|▍         | 60/1406 [01:30<33:47,  1.51s/it]  4%|▍         | 61/1406 [01:32<33:48,  1.51s/it]  4%|▍         | 62/1406 [01:33<33:45,  1.51s/it]  4%|▍         | 63/1406 [01:35<33:43,  1.51s/it]  5%|▍         | 64/1406 [01:36<33:44,  1.51s/it]  5%|▍         | 65/1406 [01:38<33:42,  1.51s/it]  5%|▍         | 66/1406 [01:39<33:45,  1.51s/it]  5%|▍         | 67/1406 [01:41<33:40,  1.51s/it]  5%|▍         | 68/1406 [01:42<33:36,  1.51s/it]  5%|▍         | 69/1406 [01:44<33:33,  1.51s/it]  5%|▍         | 70/1406 [01:45<33:31,  1.51s/it]  5%|▌         | 71/1406 [01:47<33:33,  1.51s/it]  5%|▌         | 72/1406 [01:48<33:29,  1.51s/it]  5%|▌         | 73/1406 [01:50<33:29,  1.51s/it]  5%|▌         | 74/1406 [01:51<33:26,  1.51s/it]  5%|▌         | 75/1406 [01:53<33:25,  1.51s/it]  5%|▌         | 76/1406 [01:54<33:21,  1.50s/it]  5%|▌         | 77/1406 [01:56<33:19,  1.50s/it]  6%|▌         | 78/1406 [01:57<33:18,  1.50s/it]  6%|▌         | 79/1406 [01:59<33:16,  1.50s/it]  6%|▌         | 80/1406 [02:00<33:19,  1.51s/it]  6%|▌         | 81/1406 [02:02<33:18,  1.51s/it]  6%|▌         | 82/1406 [02:03<33:16,  1.51s/it]  6%|▌         | 83/1406 [02:05<33:12,  1.51s/it]  6%|▌         | 84/1406 [02:06<33:12,  1.51s/it]  6%|▌         | 85/1406 [02:08<33:10,  1.51s/it]  6%|▌         | 86/1406 [02:09<33:07,  1.51s/it]  6%|▌         | 87/1406 [02:11<33:07,  1.51s/it]  6%|▋         | 88/1406 [02:12<33:03,  1.51s/it]  6%|▋         | 89/1406 [02:14<33:03,  1.51s/it]  6%|▋         | 90/1406 [02:16<33:01,  1.51s/it]  6%|▋         | 91/1406 [02:17<33:01,  1.51s/it]  7%|▋         | 92/1406 [02:19<33:03,  1.51s/it]  7%|▋         | 93/1406 [02:20<33:01,  1.51s/it]  7%|▋         | 94/1406 [02:22<32:57,  1.51s/it]  7%|▋         | 95/1406 [02:23<32:58,  1.51s/it]  7%|▋         | 96/1406 [02:25<32:55,  1.51s/it]  7%|▋         | 97/1406 [02:26<32:52,  1.51s/it]  7%|▋         | 98/1406 [02:28<32:49,  1.51s/it]  7%|▋         | 99/1406 [02:29<32:49,  1.51s/it]  7%|▋         | 100/1406 [02:31<32:45,  1.50s/it]  7%|▋         | 101/1406 [02:32<32:41,  1.50s/it]  7%|▋         | 102/1406 [02:34<32:39,  1.50s/it]  7%|▋         | 103/1406 [02:35<32:37,  1.50s/it]  7%|▋         | 104/1406 [02:37<32:31,  1.50s/it]  7%|▋         | 105/1406 [02:38<32:29,  1.50s/it]  8%|▊         | 106/1406 [02:40<32:26,  1.50s/it]  8%|▊         | 107/1406 [02:41<32:25,  1.50s/it]  8%|▊         | 108/1406 [02:43<32:28,  1.50s/it]  8%|▊         | 109/1406 [02:44<32:24,  1.50s/it]  8%|▊         | 110/1406 [02:46<32:22,  1.50s/it]  8%|▊         | 111/1406 [02:47<32:23,  1.50s/it]  8%|▊         | 112/1406 [02:49<32:22,  1.50s/it]  8%|▊         | 113/1406 [02:50<32:20,  1.50s/it]  8%|▊         | 114/1406 [02:52<32:15,  1.50s/it]  8%|▊         | 115/1406 [02:53<32:12,  1.50s/it]  8%|▊         | 116/1406 [02:55<32:16,  1.50s/it]  8%|▊         | 117/1406 [02:56<32:12,  1.50s/it]  8%|▊         | 118/1406 [02:58<32:12,  1.50s/it]  8%|▊         | 119/1406 [02:59<32:11,  1.50s/it]  9%|▊         | 120/1406 [03:01<32:08,  1.50s/it]  9%|▊         | 121/1406 [03:02<32:05,  1.50s/it]  9%|▊         | 122/1406 [03:04<32:04,  1.50s/it]  9%|▊         | 123/1406 [03:05<32:01,  1.50s/it]  9%|▉         | 124/1406 [03:07<32:01,  1.50s/it]  9%|▉         | 125/1406 [03:08<31:57,  1.50s/it]  9%|▉         | 126/1406 [03:10<31:55,  1.50s/it]  9%|▉         | 127/1406 [03:11<31:53,  1.50s/it]  9%|▉         | 128/1406 [03:13<31:52,  1.50s/it]  9%|▉         | 129/1406 [03:14<31:51,  1.50s/it]  9%|▉         | 130/1406 [03:16<31:48,  1.50s/it]  9%|▉         | 131/1406 [03:17<31:47,  1.50s/it]  9%|▉         | 132/1406 [03:19<31:46,  1.50s/it]  9%|▉         | 133/1406 [03:20<32:12,  1.52s/it]OPTIONS Namespace(batch_size=32, cache_dir='./cache', channels=60, data_dir='/home/ar5g15/workspace/imagenet_valid', embedding_size=300, epochs=31, kernel_size=7, latent_size=32, limit=None, lr=0.001, model='vae-up', no_gates=False, no_hv=False, no_res=False, num_layers=5, task='imagenet64', tb_dir='./runs/pixel', vae_depth=0, zsize=64)
vae-up
a fost
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
Constructed network ImEncoder(
  (encoder): Sequential(
    (0): Block(
      (upchannels): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
    (2): Block(
      (upchannels): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
    (4): Block(
      (upchannels): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
    (6): Flatten()
    (7): Linear(in_features=8192, out_features=128, bias=True)
  )
) ImDecoder(
  (decoder): Sequential(
    (0): Linear(in_features=64, out_features=8192, bias=True)
    (1): ReLU()
    (2): Reshape()
    (3): Upsample(scale_factor=2, mode=bilinear)
    (4): Block(
      (upchannels): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (5): Upsample(scale_factor=2, mode=bilinear)
    (6): Block(
      (upchannels): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (7): Upsample(scale_factor=2, mode=bilinear)
    (8): Block(
      (upchannels): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): ConvTranspose2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): ConvTranspose2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): ConvTranspose2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (9): ConvTranspose2d(60, 64, kernel_size=(1, 1), stride=(1, 1))
    (10): Sigmoid()
  )
) LGated(
  (conv1): Conv2d(3, 60, kernel_size=(1, 1), stride=(1, 1), groups=3)
  (gated_layers): ModuleList(
    (0): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (1): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (3): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (4): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (conv2): Conv2d(60, 768, kernel_size=(1, 1), stride=(1, 1), groups=3)
)
  0%|          | 0/1406 [00:00<?, ?it/s] 10%|▉         | 134/1406 [03:22<32:50,  1.55s/it] 10%|▉         | 135/1406 [03:24<36:41,  1.73s/it]/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.{} is deprecated. Use nn.functional.interpolate instead.".format(self.name))
/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  0%|          | 1/1406 [00:03<1:12:58,  3.12s/it] 10%|▉         | 136/1406 [03:26<39:48,  1.88s/it]  0%|          | 2/1406 [00:05<1:07:21,  2.88s/it] 10%|▉         | 137/1406 [03:28<42:19,  2.00s/it]  0%|          | 3/1406 [00:07<1:03:04,  2.70s/it] 10%|▉         | 138/1406 [03:31<44:00,  2.08s/it]  0%|          | 4/1406 [00:09<1:00:05,  2.57s/it] 10%|▉         | 139/1406 [03:33<45:08,  2.14s/it]  0%|          | 5/1406 [00:12<58:00,  2.48s/it]   10%|▉         | 140/1406 [03:35<45:59,  2.18s/it]  0%|          | 6/1406 [00:14<56:30,  2.42s/it] 10%|█         | 141/1406 [03:37<46:30,  2.21s/it]  0%|          | 7/1406 [00:16<55:33,  2.38s/it] 10%|█         | 142/1406 [03:40<46:54,  2.23s/it]  1%|          | 8/1406 [00:19<54:47,  2.35s/it] 10%|█         | 143/1406 [03:42<47:10,  2.24s/it]  1%|          | 9/1406 [00:21<54:14,  2.33s/it] 10%|█         | 144/1406 [03:44<47:22,  2.25s/it]  1%|          | 10/1406 [00:23<53:51,  2.31s/it] 10%|█         | 145/1406 [03:47<47:35,  2.26s/it]  1%|          | 11/1406 [00:25<53:40,  2.31s/it] 10%|█         | 146/1406 [03:49<47:45,  2.27s/it]  1%|          | 12/1406 [00:28<53:26,  2.30s/it] 10%|█         | 147/1406 [03:51<47:54,  2.28s/it]  1%|          | 13/1406 [00:30<53:20,  2.30s/it] 11%|█         | 148/1406 [03:53<47:55,  2.29s/it]  1%|          | 14/1406 [00:32<53:15,  2.30s/it] 11%|█         | 149/1406 [03:56<47:56,  2.29s/it]  1%|          | 15/1406 [00:35<53:12,  2.30s/it] 11%|█         | 150/1406 [03:58<47:58,  2.29s/it]  1%|          | 16/1406 [00:37<53:11,  2.30s/it] 11%|█         | 151/1406 [04:00<47:54,  2.29s/it]  1%|          | 17/1406 [00:39<53:15,  2.30s/it] 11%|█         | 152/1406 [04:03<47:48,  2.29s/it]  1%|▏         | 18/1406 [00:42<53:17,  2.30s/it] 11%|█         | 153/1406 [04:05<47:45,  2.29s/it]  1%|▏         | 19/1406 [00:44<53:12,  2.30s/it] 11%|█         | 154/1406 [04:07<47:45,  2.29s/it]  1%|▏         | 20/1406 [00:46<53:08,  2.30s/it] 11%|█         | 155/1406 [04:10<47:43,  2.29s/it]  1%|▏         | 21/1406 [00:48<53:05,  2.30s/it] 11%|█         | 156/1406 [04:12<47:40,  2.29s/it]  2%|▏         | 22/1406 [00:51<53:02,  2.30s/it] 11%|█         | 157/1406 [04:14<47:39,  2.29s/it]  2%|▏         | 23/1406 [00:53<52:59,  2.30s/it] 11%|█         | 158/1406 [04:16<47:37,  2.29s/it]  2%|▏         | 24/1406 [00:55<52:56,  2.30s/it] 11%|█▏        | 159/1406 [04:19<47:34,  2.29s/it]  2%|▏         | 25/1406 [00:58<52:53,  2.30s/it] 11%|█▏        | 160/1406 [04:21<47:55,  2.31s/it]  2%|▏         | 26/1406 [01:00<53:31,  2.33s/it]OPTIONS Namespace(batch_size=32, cache_dir='./cache', channels=60, data_dir='/home/ar5g15/workspace/imagenet_valid', embedding_size=300, epochs=31, kernel_size=7, latent_size=32, limit=None, lr=0.001, model='vae-up', no_gates=False, no_hv=False, no_res=False, num_layers=5, task='imagenet64', tb_dir='./runs/pixel', vae_depth=0, zsize=64)
vae-up
a fost
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
tensor([[1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        [1., 1., 1.,  ..., 0., 0., 0.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
Constructed network ImEncoder(
  (encoder): Sequential(
    (0): Block(
      (upchannels): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
    (2): Block(
      (upchannels): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
    (4): Block(
      (upchannels): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
    (6): Flatten()
    (7): Linear(in_features=8192, out_features=128, bias=True)
  )
) ImDecoder(
  (decoder): Sequential(
    (0): Linear(in_features=64, out_features=8192, bias=True)
    (1): ReLU()
    (2): Reshape()
    (3): Upsample(scale_factor=2, mode=bilinear)
    (4): Block(
      (upchannels): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (5): Upsample(scale_factor=2, mode=bilinear)
    (6): Block(
      (upchannels): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (7): Upsample(scale_factor=2, mode=bilinear)
    (8): Block(
      (upchannels): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (seq): Sequential(
        (0): ConvTranspose2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): ConvTranspose2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): ConvTranspose2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
      )
    )
    (9): ConvTranspose2d(60, 64, kernel_size=(1, 1), stride=(1, 1))
    (10): Sigmoid()
  )
) LGated(
  (conv1): Conv2d(3, 60, kernel_size=(1, 1), stride=(1, 1), groups=3)
  (gated_layers): ModuleList(
    (0): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (1): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (3): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (4): LMaskedConv2d(
      (vertical): Conv2d(60, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (horizontal): Conv2d(60, 120, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
      (tohori): Conv2d(120, 120, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (tores): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1), groups=3, bias=False)
      (vhf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vhg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvf): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
      (vvg): Conv2d(64, 60, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (conv2): Conv2d(60, 768, kernel_size=(1, 1), stride=(1, 1), groups=3)
)
  0%|          | 0/1406 [00:00<?, ?it/s]/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.{} is deprecated. Use nn.functional.interpolate instead.".format(self.name))
/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")

Traceback (most recent call last):
  File "train-vae.py", line 427, in <module>
    go(options)
  File "train-vae.py", line 239, in go
    rec = pixcnn(input, out)
  File "/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ar5g15/workspace/master/pixel-models-master/models.py", line 77, in forward
    xv, xh = layer(xv, xh, cond)
  File "/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ar5g15/workspace/master/pixel-models-master/layers.py", line 314, in forward
    hx = hx + self.tohori(vx)
  File "/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ar5g15/miniconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 320, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 11.93 GiB total capacity; 1.45 GiB already allocated; 11.81 MiB free; 103.72 MiB cached)
 11%|█▏        | 161/1406 [04:23<48:55,  2.36s/it]  2%|▏         | 27/1406 [01:02<54:18,  2.36s/it] 12%|█▏        | 162/1406 [04:26<48:32,  2.34s/it]  2%|▏         | 28/1406 [01:05<53:49,  2.34s/it] 12%|█▏        | 163/1406 [04:28<48:09,  2.32s/it]  2%|▏         | 29/1406 [01:07<53:33,  2.33s/it] 12%|█▏        | 164/1406 [04:30<47:51,  2.31s/it]  2%|▏         | 30/1406 [01:09<53:17,  2.32s/it] 12%|█▏        | 165/1406 [04:33<47:39,  2.30s/it]  2%|▏         | 31/1406 [01:12<53:06,  2.32s/it] 12%|█▏        | 166/1406 [04:35<47:32,  2.30s/it]  2%|▏         | 32/1406 [01:14<52:56,  2.31s/it] 12%|█▏        | 167/1406 [04:37<47:30,  2.30s/it]  2%|▏         | 33/1406 [01:16<52:44,  2.30s/it] 12%|█▏        | 168/1406 [04:40<47:23,  2.30s/it]  2%|▏         | 34/1406 [01:19<52:42,  2.31s/it] 12%|█▏        | 169/1406 [04:42<47:19,  2.30s/it]  2%|▏         | 35/1406 [01:21<52:38,  2.30s/it] 12%|█▏        | 170/1406 [04:44<47:17,  2.30s/it]  3%|▎         | 36/1406 [01:23<52:31,  2.30s/it] 12%|█▏        | 171/1406 [04:46<47:17,  2.30s/it]  3%|▎         | 37/1406 [01:25<52:25,  2.30s/it] 12%|█▏        | 172/1406 [04:49<47:14,  2.30s/it]  3%|▎         | 38/1406 [01:28<52:20,  2.30s/it] 12%|█▏        | 173/1406 [04:51<47:09,  2.29s/it]  3%|▎         | 39/1406 [01:30<52:20,  2.30s/it] 12%|█▏        | 174/1406 [04:53<47:05,  2.29s/it]  3%|▎         | 40/1406 [01:32<52:18,  2.30s/it] 12%|█▏        | 175/1406 [04:56<47:01,  2.29s/it]  3%|▎         | 41/1406 [01:35<52:14,  2.30s/it] 13%|█▎        | 176/1406 [04:58<47:01,  2.29s/it]  3%|▎         | 42/1406 [01:37<52:15,  2.30s/it] 13%|█▎        | 177/1406 [05:00<46:53,  2.29s/it]  3%|▎         | 43/1406 [01:39<52:15,  2.30s/it] 13%|█▎        | 178/1406 [05:02<46:51,  2.29s/it]  3%|▎         | 44/1406 [01:42<52:11,  2.30s/it] 13%|█▎        | 179/1406 [05:05<46:48,  2.29s/it]  3%|▎         | 45/1406 [01:44<52:11,  2.30s/it] 13%|█▎        | 180/1406 [05:07<46:47,  2.29s/it]  3%|▎         | 46/1406 [01:46<52:06,  2.30s/it] 13%|█▎        | 181/1406 [05:09<46:51,  2.29s/it]  3%|▎         | 47/1406 [01:48<51:58,  2.29s/it] 13%|█▎        | 182/1406 [05:12<46:48,  2.29s/it]  3%|▎         | 48/1406 [01:51<51:57,  2.30s/it] 13%|█▎        | 183/1406 [05:14<46:39,  2.29s/it]  3%|▎         | 49/1406 [01:53<52:03,  2.30s/it] 13%|█▎        | 184/1406 [05:16<46:37,  2.29s/it]  4%|▎         | 50/1406 [01:55<51:59,  2.30s/it] 13%|█▎        | 185/1406 [05:18<46:36,  2.29s/it]  4%|▎         | 51/1406 [01:58<51:59,  2.30s/it] 13%|█▎        | 186/1406 [05:21<46:31,  2.29s/it]  4%|▎         | 52/1406 [02:00<51:55,  2.30s/it] 13%|█▎        | 187/1406 [05:23<46:25,  2.28s/it]  4%|▍         | 53/1406 [02:02<51:57,  2.30s/it] 13%|█▎        | 188/1406 [05:25<46:28,  2.29s/it]  4%|▍         | 54/1406 [02:05<51:48,  2.30s/it] 13%|█▎        | 189/1406 [05:28<46:27,  2.29s/it]  4%|▍         | 55/1406 [02:07<51:47,  2.30s/it] 14%|█▎        | 190/1406 [05:30<46:26,  2.29s/it]  4%|▍         | 56/1406 [02:09<51:45,  2.30s/it] 14%|█▎        | 191/1406 [05:32<46:22,  2.29s/it]  4%|▍         | 57/1406 [02:11<51:45,  2.30s/it] 14%|█▎        | 192/1406 [05:35<46:18,  2.29s/it]  4%|▍         | 58/1406 [02:14<51:47,  2.31s/it] 14%|█▎        | 193/1406 [05:37<46:15,  2.29s/it]  4%|▍         | 59/1406 [02:16<51:41,  2.30s/it] 14%|█▍        | 194/1406 [05:39<46:16,  2.29s/it]  4%|▍         | 60/1406 [02:18<51:39,  2.30s/it] 14%|█▍        | 195/1406 [05:41<46:15,  2.29s/it]  4%|▍         | 61/1406 [02:21<51:37,  2.30s/it] 14%|█▍        | 196/1406 [05:44<46:13,  2.29s/it]  4%|▍         | 62/1406 [02:23<51:34,  2.30s/it] 14%|█▍        | 197/1406 [05:46<46:10,  2.29s/it]  4%|▍         | 63/1406 [02:25<51:30,  2.30s/it] 14%|█▍        | 198/1406 [05:48<46:10,  2.29s/it]  5%|▍         | 64/1406 [02:28<51:45,  2.31s/it] 14%|█▍        | 199/1406 [05:51<46:46,  2.33s/it]  5%|▍         | 65/1406 [02:30<52:06,  2.33s/it] 14%|█▍        | 200/1406 [05:53<47:23,  2.36s/it]  5%|▍         | 66/1406 [02:32<52:26,  2.35s/it] 14%|█▍        | 201/1406 [05:55<46:58,  2.34s/it]  5%|▍         | 67/1406 [02:35<52:04,  2.33s/it] 14%|█▍        | 202/1406 [05:58<46:35,  2.32s/it]  5%|▍         | 68/1406 [02:37<51:50,  2.33s/it] 14%|█▍        | 203/1406 [06:00<46:20,  2.31s/it]  5%|▍         | 69/1406 [02:39<51:39,  2.32s/it] 15%|█▍        | 204/1406 [06:02<46:11,  2.31s/it]  5%|▍         | 70/1406 [02:42<51:26,  2.31s/it] 15%|█▍        | 205/1406 [06:05<46:02,  2.30s/it]  5%|▌         | 71/1406 [02:44<51:18,  2.31s/it] 15%|█▍        | 206/1406 [06:07<45:54,  2.30s/it]  5%|▌         | 72/1406 [02:46<51:16,  2.31s/it] 15%|█▍        | 207/1406 [06:09<45:47,  2.29s/it]  5%|▌         | 73/1406 [02:48<51:11,  2.30s/it] 15%|█▍        | 208/1406 [06:11<45:47,  2.29s/it]  5%|▌         | 74/1406 [02:51<51:08,  2.30s/it] 15%|█▍        | 209/1406 [06:14<45:43,  2.29s/it]  5%|▌         | 75/1406 [02:53<51:05,  2.30s/it] 15%|█▍        | 210/1406 [06:16<45:41,  2.29s/it]  5%|▌         | 76/1406 [02:55<51:02,  2.30s/it] 15%|█▌        | 211/1406 [06:18<45:38,  2.29s/it]  5%|▌         | 77/1406 [02:58<50:57,  2.30s/it] 15%|█▌        | 212/1406 [06:21<45:36,  2.29s/it]  6%|▌         | 78/1406 [03:00<50:56,  2.30s/it] 15%|█▌        | 213/1406 [06:23<45:33,  2.29s/it]  6%|▌         | 79/1406 [03:02<50:53,  2.30s/it] 15%|█▌        | 214/1406 [06:25<45:25,  2.29s/it]  6%|▌         | 80/1406 [03:05<50:54,  2.30s/it] 15%|█▌        | 215/1406 [06:27<45:21,  2.29s/it]  6%|▌         | 81/1406 [03:07<50:53,  2.30s/it] 15%|█▌        | 216/1406 [06:30<45:24,  2.29s/it]  6%|▌         | 82/1406 [03:09<50:48,  2.30s/it] 15%|█▌        | 217/1406 [06:32<45:17,  2.29s/it]  6%|▌         | 83/1406 [03:12<50:48,  2.30s/it] 16%|█▌        | 218/1406 [06:34<45:14,  2.28s/it]  6%|▌         | 84/1406 [03:14<50:46,  2.30s/it] 16%|█▌        | 219/1406 [06:37<45:15,  2.29s/it]  6%|▌         | 85/1406 [03:16<50:41,  2.30s/it] 16%|█▌        | 220/1406 [06:39<45:17,  2.29s/it]  6%|▌         | 86/1406 [03:18<50:34,  2.30s/it] 16%|█▌        | 221/1406 [06:41<45:13,  2.29s/it]  6%|▌         | 87/1406 [03:21<50:34,  2.30s/it] 16%|█▌        | 222/1406 [06:43<45:11,  2.29s/it]  6%|▋         | 88/1406 [03:23<50:28,  2.30s/it] 16%|█▌        | 223/1406 [06:46<45:13,  2.29s/it]  6%|▋         | 89/1406 [03:25<50:27,  2.30s/it] 16%|█▌        | 224/1406 [06:48<45:09,  2.29s/it]  6%|▋         | 90/1406 [03:28<50:25,  2.30s/it] 16%|█▌        | 225/1406 [06:50<45:08,  2.29s/it]  6%|▋         | 91/1406 [03:30<50:24,  2.30s/it] 16%|█▌        | 226/1406 [06:53<45:08,  2.29s/it]  7%|▋         | 92/1406 [03:32<50:17,  2.30s/it] 16%|█▌        | 227/1406 [06:55<45:07,  2.30s/it]  7%|▋         | 93/1406 [03:34<50:16,  2.30s/it] 16%|█▌        | 228/1406 [06:57<45:02,  2.29s/it]  7%|▋         | 94/1406 [03:37<50:10,  2.29s/it] 16%|█▋        | 229/1406 [07:00<45:03,  2.30s/it]  7%|▋         | 95/1406 [03:39<50:11,  2.30s/it] 16%|█▋        | 230/1406 [07:02<44:57,  2.29s/it]  7%|▋         | 96/1406 [03:41<50:10,  2.30s/it] 16%|█▋        | 231/1406 [07:04<44:57,  2.30s/it]  7%|▋         | 97/1406 [03:44<50:08,  2.30s/it] 17%|█▋        | 232/1406 [07:06<44:52,  2.29s/it]  7%|▋         | 98/1406 [03:46<50:11,  2.30s/it] 17%|█▋        | 233/1406 [07:09<44:46,  2.29s/it]  7%|▋         | 99/1406 [03:48<50:07,  2.30s/it] 17%|█▋        | 234/1406 [07:11<44:46,  2.29s/it]  7%|▋         | 100/1406 [03:51<50:04,  2.30s/it] 17%|█▋        | 235/1406 [07:13<44:43,  2.29s/it]  7%|▋         | 101/1406 [03:53<50:00,  2.30s/it] 17%|█▋        | 236/1406 [07:16<44:43,  2.29s/it]  7%|▋         | 102/1406 [03:55<49:55,  2.30s/it] 17%|█▋        | 237/1406 [07:18<44:42,  2.29s/it]  7%|▋         | 103/1406 [03:57<49:56,  2.30s/it] 17%|█▋        | 238/1406 [07:20<44:34,  2.29s/it]  7%|▋         | 104/1406 [04:00<49:53,  2.30s/it] 17%|█▋        | 239/1406 [07:22<44:35,  2.29s/it]  7%|▋         | 105/1406 [04:02<49:51,  2.30s/it] 17%|█▋        | 240/1406 [07:25<44:33,  2.29s/it]  8%|▊         | 106/1406 [04:04<49:46,  2.30s/it] 17%|█▋        | 241/1406 [07:27<44:35,  2.30s/it]  8%|▊         | 107/1406 [04:07<49:42,  2.30s/it] 17%|█▋        | 242/1406 [07:29<44:30,  2.29s/it]  8%|▊         | 108/1406 [04:09<49:44,  2.30s/it] 17%|█▋        | 243/1406 [07:32<44:28,  2.29s/it]  8%|▊         | 109/1406 [04:11<49:37,  2.30s/it] 17%|█▋        | 244/1406 [07:34<44:27,  2.30s/it]  8%|▊         | 110/1406 [04:14<49:34,  2.30s/it] 17%|█▋        | 245/1406 [07:36<44:24,  2.29s/it]  8%|▊         | 111/1406 [04:16<49:31,  2.29s/it] 17%|█▋        | 246/1406 [07:39<44:22,  2.30s/it]  8%|▊         | 112/1406 [04:18<49:29,  2.29s/it] 18%|█▊        | 247/1406 [07:41<44:22,  2.30s/it]  8%|▊         | 113/1406 [04:20<49:26,  2.29s/it] 18%|█▊        | 248/1406 [07:43<44:20,  2.30s/it]  8%|▊         | 114/1406 [04:23<49:26,  2.30s/it] 18%|█▊        | 249/1406 [07:45<44:17,  2.30s/it]  8%|▊         | 115/1406 [04:25<49:24,  2.30s/it] 18%|█▊        | 250/1406 [07:48<44:14,  2.30s/it]  8%|▊         | 116/1406 [04:27<49:21,  2.30s/it] 18%|█▊        | 251/1406 [07:50<44:13,  2.30s/it]  8%|▊         | 117/1406 [04:30<49:20,  2.30s/it] 18%|█▊        | 252/1406 [07:52<44:08,  2.30s/it]  8%|▊         | 118/1406 [04:32<49:18,  2.30s/it] 18%|█▊        | 253/1406 [07:55<44:05,  2.29s/it]  8%|▊         | 119/1406 [04:34<49:15,  2.30s/it] 18%|█▊        | 254/1406 [07:57<44:02,  2.29s/it]  9%|▊         | 120/1406 [04:37<49:15,  2.30s/it] 18%|█▊        | 255/1406 [07:59<43:59,  2.29s/it]  9%|▊         | 121/1406 [04:39<49:15,  2.30s/it] 18%|█▊        | 256/1406 [08:01<43:56,  2.29s/it]  9%|▊         | 122/1406 [04:41<49:11,  2.30s/it] 18%|█▊        | 257/1406 [08:04<43:55,  2.29s/it]  9%|▊         | 123/1406 [04:43<49:08,  2.30s/it] 18%|█▊        | 258/1406 [08:06<43:52,  2.29s/it]  9%|▉         | 124/1406 [04:46<49:07,  2.30s/it] 18%|█▊        | 259/1406 [08:08<43:47,  2.29s/it]  9%|▉         | 125/1406 [04:48<49:02,  2.30s/it] 18%|█▊        | 260/1406 [08:11<43:50,  2.30s/it]  9%|▉         | 126/1406 [04:50<48:58,  2.30s/it] 19%|█▊        | 261/1406 [08:13<43:48,  2.30s/it]  9%|▉         | 127/1406 [04:53<48:59,  2.30s/it] 19%|█▊        | 262/1406 [08:15<43:43,  2.29s/it]  9%|▉         | 128/1406 [04:55<49:01,  2.30s/it] 19%|█▊        | 263/1406 [08:18<43:41,  2.29s/it]  9%|▉         | 129/1406 [04:57<48:55,  2.30s/it] 19%|█▉        | 264/1406 [08:20<43:40,  2.29s/it]  9%|▉         | 130/1406 [05:00<48:52,  2.30s/it] 19%|█▉        | 265/1406 [08:22<43:38,  2.29s/it]  9%|▉         | 131/1406 [05:02<48:47,  2.30s/it] 19%|█▉        | 266/1406 [08:24<43:34,  2.29s/it]  9%|▉         | 132/1406 [05:04<48:48,  2.30s/it] 19%|█▉        | 267/1406 [08:27<43:34,  2.30s/it]  9%|▉         | 133/1406 [05:06<48:42,  2.30s/it] 19%|█▉        | 268/1406 [08:29<43:32,  2.30s/it] 10%|▉         | 134/1406 [05:09<48:40,  2.30s/it] 19%|█▉        | 269/1406 [08:31<43:30,  2.30s/it] 10%|▉         | 135/1406 [05:11<48:39,  2.30s/it] 19%|█▉        | 270/1406 [08:34<43:26,  2.29s/it] 10%|▉         | 136/1406 [05:13<48:36,  2.30s/it] 19%|█▉        | 271/1406 [08:36<43:23,  2.29s/it] 10%|▉         | 137/1406 [05:16<48:31,  2.29s/it] 19%|█▉        | 272/1406 [08:38<43:22,  2.30s/it] 10%|▉         | 138/1406 [05:18<48:32,  2.30s/it] 19%|█▉        | 273/1406 [08:40<43:20,  2.30s/it] 10%|▉         | 139/1406 [05:20<48:31,  2.30s/it] 19%|█▉        | 274/1406 [08:43<43:15,  2.29s/it] 10%|▉         | 140/1406 [05:22<48:30,  2.30s/it] 20%|█▉        | 275/1406 [08:45<43:17,  2.30s/it] 10%|█         | 141/1406 [05:25<48:22,  2.29s/it] 20%|█▉        | 276/1406 [08:47<43:17,  2.30s/it] 10%|█         | 142/1406 [05:27<48:18,  2.29s/it] 20%|█▉        | 277/1406 [08:50<43:13,  2.30s/it] 10%|█         | 143/1406 [05:29<48:16,  2.29s/it] 20%|█▉        | 278/1406 [08:52<43:11,  2.30s/it] 10%|█         | 144/1406 [05:32<48:15,  2.29s/it] 20%|█▉        | 279/1406 [08:54<43:07,  2.30s/it] 10%|█         | 145/1406 [05:34<48:16,  2.30s/it] 20%|█▉        | 280/1406 [08:57<43:03,  2.29s/it] 10%|█         | 146/1406 [05:36<48:15,  2.30s/it] 20%|█▉        | 281/1406 [08:59<43:06,  2.30s/it] 10%|█         | 147/1406 [05:39<48:08,  2.29s/it] 20%|██        | 282/1406 [09:01<43:03,  2.30s/it] 11%|█         | 148/1406 [05:41<48:07,  2.30s/it] 20%|██        | 283/1406 [09:03<42:58,  2.30s/it] 11%|█         | 149/1406 [05:43<48:07,  2.30s/it] 20%|██        | 284/1406 [09:06<42:56,  2.30s/it] 11%|█         | 150/1406 [05:45<48:03,  2.30s/it] 20%|██        | 285/1406 [09:08<42:57,  2.30s/it] 11%|█         | 151/1406 [05:48<47:58,  2.29s/it] 20%|██        | 286/1406 [09:10<42:54,  2.30s/it] 11%|█         | 152/1406 [05:50<47:58,  2.30s/it] 20%|██        | 287/1406 [09:13<42:50,  2.30s/it] 11%|█         | 153/1406 [05:52<47:56,  2.30s/it] 20%|██        | 288/1406 [09:15<42:47,  2.30s/it] 11%|█         | 154/1406 [05:55<47:58,  2.30s/it] 21%|██        | 289/1406 [09:17<42:41,  2.29s/it] 11%|█         | 155/1406 [05:57<45:46,  2.20s/it] 11%|█         | 156/1406 [05:58<41:17,  1.98s/it] 11%|█         | 157/1406 [06:00<38:08,  1.83s/it]